{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EGDDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, use_test_data=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load training data\n",
    "        train_dir = os.path.join(root_dir, 'train')\n",
    "        for label in os.listdir(train_dir):\n",
    "            label_path = os.path.join(train_dir, label)\n",
    "            for file_name in os.listdir(label_path):\n",
    "                if file_name.endswith('.BMP'):\n",
    "                    file_path = os.path.join(label_path, file_name)\n",
    "                    self.data.append(file_path)\n",
    "                    self.labels.append(int(label))\n",
    "\n",
    "        # Optionally load test data\n",
    "        if use_test_data:\n",
    "            test_dir = os.path.join(root_dir, 'test')\n",
    "            for label in os.listdir(test_dir):\n",
    "                label_path = os.path.join(test_dir, label)\n",
    "                for file_name in os.listdir(label_path):\n",
    "                    if file_name.endswith('.BMP'):\n",
    "                        file_path = os.path.join(label_path, file_name)\n",
    "                        self.data.append(file_path)\n",
    "                        self.labels.append(int(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Image loading\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "def get_EGD_dataloader(root_dir, batch_size, transforms, shuffle=True):\n",
    "    dataset = EGDDataset(root_dir=root_dir, transform=transforms)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# dataloader.py\\nimport os\\nfrom torchvision import transforms\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom PIL import Image\\nimport matplotlib.pyplot as plt\\n\\n\\nclass_mapping = {'paella': 0, 'macaroni_and_cheese': 1, 'waffles': 2}\\n\\nclass CustomFoodDataset(Dataset):\\n    def __init__(self, root_dir, transform=None):\\n        self.root_dir = root_dir\\n        self.transform = transform\\n        self.images = []\\n        self.labels = []\\n        \\n        # Load images and labels\\n        for class_name, class_idx in class_mapping.items():\\n            class_dir = os.path.join(root_dir, 'images', class_name)\\n            for img_name in os.listdir(class_dir):\\n                self.images.append(os.path.join(class_dir, img_name))\\n                self.labels.append(class_idx)\\n\\n    def __len__(self):\\n        return len(self.images)\\n\\n    def __getitem__(self, idx):\\n        img_path = self.images[idx]\\n        image = Image.open(img_path).convert('RGB')\\n        label = self.labels[idx]\\n\\n        if self.transform:\\n            image = self.transform(image)\\n\\n        return image, label\\n\\ndef get_food101_dataloader(batch_size, root_dir, transform):\\n    dataset = CustomFoodDataset(root_dir=root_dir, transform=transform)\\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\\n\\n# Define transformations\\ntransform = transforms.Compose([\\n    transforms.Resize((224, 224)),\\n    transforms.RandomHorizontalFlip(p=0.5),\\n    transforms.RandomVerticalFlip(p=0.5),\\n    transforms.RandomRotation(degrees=10),\\n    transforms.ToTensor(),\\n])\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# dataloader.py\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class_mapping = {'paella': 0, 'macaroni_and_cheese': 1, 'waffles': 2}\n",
    "\n",
    "class CustomFoodDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load images and labels\n",
    "        for class_name, class_idx in class_mapping.items():\n",
    "            class_dir = os.path.join(root_dir, 'images', class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                self.images.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_food101_dataloader(batch_size, root_dir, transform):\n",
    "    dataset = CustomFoodDataset(root_dir=root_dir, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\nclass ConvVAE(nn.Module):\\n    def __init__(self):\\n        super(ConvVAE, self).__init__()\\n\\n        # Encoder\\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # 16x224x224\\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 16x112x112\\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # 32x112x112\\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 32x56x56\\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 64x56x56\\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x28x28\\n        # Optional: You can continue to reduce the number of layers or filters based on memory constraints\\n\\n        # To compute mu and logvar of the latent vector z we use a convolutional layer\\n        self.mu_conv = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  # Adjusted number of filters\\n        self.logvar_conv = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\\n\\n        # Decoder\\n        self.deconv1 = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\\n        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\\n        self.deconv3 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\\n        # Adjust the final layer to ensure correct output size\\n        self.deconv4 = nn.ConvTranspose2d(16, 3, kernel_size=3, stride=1, padding=1)\\n        \\n    def encode(self, x):\\n        x = F.relu(self.conv1(x))\\n        x = self.maxpool1(x)\\n        x = F.relu(self.conv2(x))\\n        x = self.maxpool2(x)\\n        x = F.relu(self.conv3(x))\\n        x = self.maxpool3(x)\\n        \\n        mu = self.mu_conv(x) \\n        logvar = self.logvar_conv(x) \\n        return mu, logvar\\n        \\n        \\n        \\n    def reparametrize(self, mu, logvar):\\n        std = torch.exp(0.5 * logvar)\\n        eps = torch.randn_like(std)\\n        return mu + eps * std\\n\\n    def decode(self, z):\\n        x = F.relu(self.deconv1(z))  # 56x56\\n\\n        x = F.relu(self.deconv2(x))  # 112x112\\n\\n        x = F.relu(self.deconv3(x))  # 224x224\\n\\n        x = torch.sigmoid(self.deconv4(x))  # 224x224\\n\\n        return x\\n    \\n\\n    def forward(self, x):\\n        mu, logvar = self.encode(x)\\n        z = self.reparametrize(mu, logvar)\\n        return self.decode(z), mu, logvar\\n    \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''    \n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # 16x224x224\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 16x112x112\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # 32x112x112\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 32x56x56\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 64x56x56\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x28x28\n",
    "        # Optional: You can continue to reduce the number of layers or filters based on memory constraints\n",
    "\n",
    "        # To compute mu and logvar of the latent vector z we use a convolutional layer\n",
    "        self.mu_conv = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  # Adjusted number of filters\n",
    "        self.logvar_conv = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        # Adjust the final layer to ensure correct output size\n",
    "        self.deconv4 = nn.ConvTranspose2d(16, 3, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        mu = self.mu_conv(x) \n",
    "        logvar = self.logvar_conv(x) \n",
    "        return mu, logvar\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = F.relu(self.deconv1(z))  # 56x56\n",
    "\n",
    "        x = F.relu(self.deconv2(x))  # 112x112\n",
    "\n",
    "        x = F.relu(self.deconv3(x))  # 224x224\n",
    "\n",
    "        x = torch.sigmoid(self.deconv4(x))  # 224x224\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvVAE(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (att1): SelfAttention(\n",
      "    (query_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (key_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (value_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (att2): SelfAttention(\n",
      "    (query_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (key_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (value_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (fc_mu): Linear(in_features=100352, out_features=256, bias=True)\n",
      "  (fc_logvar): Linear(in_features=100352, out_features=256, bias=True)\n",
      "  (fc_decode): Linear(in_features=256, out_features=100352, bias=True)\n",
      "  (deconv1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv3): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)  # B X N X C\n",
    "        key = self.key_conv(x).view(batch_size, -1, width * height)  # B X C x N\n",
    "        energy = torch.bmm(query, key)  # Batch matrix multiplication, B X N X N\n",
    "        attention_map = torch.softmax(energy, dim=-1)  # Softmax over the last dimension to create attention map\n",
    "        value = self.value_conv(x).view(batch_size, -1, width * height)  # B X C X N\n",
    "        out = torch.bmm(value, attention_map.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = self.gamma * out + x  # Apply attention gamma and add input\n",
    "        return out, attention_map\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.att1 = SelfAttention(in_dim=32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.att2 = SelfAttention(in_dim=64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.fc_logvar = nn.Linear(128 * 28 * 28, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(256, 128 * 28 * 28)\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x, attention_map1 = self.att1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x, attention_map2 = self.att2(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.view(-1, 128 * 28 * 28)  # Flatten the convolutional layer output\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "        \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.relu(self.fc_decode(z))\n",
    "        x = x.view(-1, 128, 28, 28)  # Unflatten to prepare for transposed convolutions\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.sigmoid(self.deconv3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Now, let's instantiate and view the model to ensure it's structured correctly.\n",
    "conv_vae = ConvVAE()\n",
    "print(conv_vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvVAE loss function\n",
    "#Loss will be the sum of the reconstruction loss and the KL divergence loss\n",
    "#The reconstruction loss will be Mean Squared Error (MSE) loss\n",
    "#The KL divergence loss will be the KL divergence between the learned mean and variance and the prior Gaussian distribution\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Normalized MSE loss\n",
    "    recon_loss = F.mse_loss(recon_x.view(-1, 3*224*224), x.view(-1, 3*224*224), reduction='sum') / x.size(0)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    \n",
    "    return recon_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "def log_images(original, label, reconstructed):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Original image\n",
    "    axs[0].imshow(original.permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n",
    "    axs[0].set_title(f'Original Image, Label: {label}')\n",
    "    # Reconstructed image\n",
    "    axs[1].imshow(reconstructed.permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n",
    "    axs[1].set_title('Reconstructed Image')\n",
    "    plt.axis('off')\n",
    "    wandb.log({\"Original vs reconstructed image\": plt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneildlf\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ndelafuente/VAE_food101/wandb/run-20231201_092450-wabvl21v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neildlf/vae_egd/runs/wabvl21v' target=\"_blank\">silver-shape-6</a></strong> to <a href='https://wandb.ai/neildlf/vae_egd' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neildlf/vae_egd' target=\"_blank\">https://wandb.ai/neildlf/vae_egd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neildlf/vae_egd/runs/wabvl21v' target=\"_blank\">https://wandb.ai/neildlf/vae_egd/runs/wabvl21v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 3171.4462890625\n",
      "====> Epoch: 0, Average loss: 2176.082705145013\n",
      "Epoch: 1, Batch: 0, Loss: 1452.3004150390625\n",
      "====> Epoch: 1, Average loss: 996.5808699098352\n",
      "Epoch: 2, Batch: 0, Loss: 701.3031616210938\n",
      "====> Epoch: 2, Average loss: 643.1130433801102\n",
      "Epoch: 3, Batch: 0, Loss: 515.1572875976562\n",
      "====> Epoch: 3, Average loss: 515.4676200135114\n",
      "Epoch: 4, Batch: 0, Loss: 544.792724609375\n",
      "====> Epoch: 4, Average loss: 457.6659721479024\n",
      "Epoch: 5, Batch: 0, Loss: 360.4516906738281\n",
      "====> Epoch: 5, Average loss: 412.8030583629869\n",
      "Epoch: 6, Batch: 0, Loss: 492.7398376464844\n",
      "====> Epoch: 6, Average loss: 386.81776365515304\n",
      "Epoch: 7, Batch: 0, Loss: 310.71649169921875\n",
      "====> Epoch: 7, Average loss: 362.12177809623824\n",
      "Epoch: 8, Batch: 0, Loss: 314.9486999511719\n",
      "====> Epoch: 8, Average loss: 340.6172934754254\n",
      "Epoch: 9, Batch: 0, Loss: 297.1343078613281\n",
      "====> Epoch: 9, Average loss: 326.68792390170165\n",
      "Epoch: 10, Batch: 0, Loss: 389.5758056640625\n",
      "====> Epoch: 10, Average loss: 329.0427701767177\n",
      "Epoch: 11, Batch: 0, Loss: 347.82110595703125\n"
     ]
    }
   ],
   "source": [
    "# Define number of epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model\n",
    "model = ConvVAE()\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "#learning rate\n",
    "lr = 3e-4\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Create dataloader\n",
    "root_dir = '/home/ndelafuente/VAE_food101/data_egd'\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_loader = get_EGD_dataloader(root_dir=root_dir, batch_size=batch_size, transforms=transform)\n",
    "\n",
    "#Initialize wandb\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"vae_egd\", entity=\"neildlf\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()/len(data)}')\n",
    "            # Visualize the first image in the batch\n",
    "            \n",
    "            label = label[0] # Get the label for the first image in the batch which is what we will visualize\n",
    "            log_images(data[0], label.item(), recon_batch[0])\n",
    "            # Log the loss\n",
    "            wandb.log({\"loss\": loss.item()/len(data)})\n",
    "            \n",
    "    # Print average loss for the epoch\n",
    "    print(f'====> Epoch: {epoch}, Average loss: {train_loss / len(train_loader.dataset)}')\n",
    "    wandb.log({\"average_loss\": train_loss / len(train_loader.dataset)})\n",
    "    # Save the model every 2 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save(model.state_dict(), f'vae_egd_epoch_{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu shape: torch.Size([1, 64, 28, 28])\n",
      "logvar shape: torch.Size([1, 64, 28, 28])\n",
      "z shape: torch.Size([1, 64, 28, 28])\n",
      "x shape after deconv1: torch.Size([1, 64, 56, 56])\n",
      "x shape after deconv2: torch.Size([1, 32, 112, 112])\n",
      "x shape after deconv3: torch.Size([1, 16, 224, 224])\n",
      "x shape after deconv4: torch.Size([1, 3, 224, 224])\n",
      "Reconstructed size: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model = ConvVAE()\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # Batch size of 1\n",
    "recon, mu, logvar = model(dummy_input)\n",
    "print(\"Reconstructed size:\", recon.size())  # Should be [1, 3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
